---
title: "Capstone: Movielens Project"
author: "Suhaimi William Chan"
date: "9/15/2020"
output:
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 1. Introduction

Recommendation systems plays an important role in e-commerce and online streaming services, such as Netflix, YouTube, Amazon, etc. Making the right recommendation for the next product/service, music or movie increases user retention and satisfaction, leading to product/service sales, thus increasing profit growth. Companies competing for customer loyalty invest on recommendation systems that capture and analyze the user’s preferences, and offer products/services with higher likelihood of purchase.

The economic impact of such company-customer relationship is very clear.  Amazon is the largest online retail company by sales and part of its success comes from the recommendation systems and marketing based on user preferences. In 2006 Netflix offered a one million dollar prize for the person or group that could improve their recommendation system by at least 10%.

Most recommendation systems are based on a rating scale from 1 to 5 grades or stars, with 1 indicating lowest satisfaction and 5 is the highest satisfaction. Other indicators can also be used, such as comments posted on previously used items; videos, musics or links shared with friends; percentage of movie watched or music listened; web pages visited and time spent on each page; product category; and any other interactions with the company’s web site or application can be used as a predictor.

The primary goal of recommendation systems is to help users find what they want based on their preferences and previous interactions, and predicting the rating for a new item. In this document, we create a movie recommendation system using the MovieLens dataset and applying the courses/lessons learned during the HarvardX’s Data Science Professional Certificate program.

This document is structured as follows: 
Chapter 1 describes the dataset and summarizes the goal of the project and key steps that were performed. 
Chapter 2 we explain the method, process and techniques used, such as data cleaning, data exploration and visualization, any insights gained, and the modeling approach. 
Chapter 3 we present the modeling analysis. 
Chapter 4 We discuss the results and model performance. 
Chapter 5 We conclude with a brief summary of the report, its limitations and future work.

The goal of this movielens project is to predict the movie ratings by users as close as the true ratings in the validation set using RMSE.
Our goal is to get our final predicted RMSE < 0.86490

We are going to use the following library:

```{r loading-libs, message=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
```


## 2. Data wrangling/data preparation and data exploration
Movielens Project is 10M dataset from [Movielens] (http://files.grouplens.org/datasets/movielens/ml-10m.zip).  
The zip file data were downloaded into our local computer system. 
There were two set of data files from the downloaded zip file. 
One is ratings.dat that contains userId, movieID, rating, and timestamp data. The other one is movies.dat that contains movieId, title, and genres data.

We stored the second data set of movies in data frame movies.  
Then we stored the first data set of ratings in data frame movielens that we used left join to data frame movies by movieId.  
Then we partitioned movielens into edx and temp with 90%/10% split, respectively.  
We created a validation data set using data frame temp that semi join to edx by userId and movieId to make sure that userId and movieId in validation set are also in edx set. 
Then we add rows removed from validation set back into edx set.

```{r wrangle and load the data, echo=FALSE, message=FALSE}
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

First, we are going the check dimension of our edx data set.

```{r check edx dimension, echo=FALSE, message=FALSE}
# We can see edx train set (about 90% of original data) dimension
dim(edx)
```

Second, we are going the check dimension of our validation data set.

```{r check validation dimension, echo=FALSE, message=FALSE}
# We can see validation set (about 10% of original data) dimension
dim(validation)
```

After making sure the data source looks good, now we are going to split edx data set into our train_set and test_set with 90/10 ratio, respectively.

```{r split edx data into train_set and test_set, echo=FALSE, message=FALSE}
# Create a training set (90% of edx) and a test_set (10% of edx data)
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
temp <- edx[test_index,]

# Make sure userId and movieId in test set are also in train set
test_set <- temp %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

# Add rows removed from test set back into train set
removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)

rm(test_index, temp, removed)
```

Now, we are going the check dimension of our train_set. (about 90% of edx data set)

```{r check train_set dimension, echo=FALSE}
# We can see our train_set (about 90% of edx data) dimension
dim(train_set)
```

Then, we are going the check dimension of our test_set.  (about 10% of edx data set)

```{r check test_set dimension, echo=FALSE}
# We can see our test_set (about 10% of edx data) dimension
dim(test_set)
```

To see a more complete data set, we are going to do our data exploration using edx data set, instead of using training set.  We can see some examples of our edx data set with available columns.

```{r check edx data sample, echo=FALSE}
# Now we are doing our data exploration using edx data set, to see a more complete data set, instead of using train set
# We can see some examples of our edx data set with available columns
head(edx)
```

First, we are going to check the classes of our edx data set columns

```{r check edx class, echo=FALSE}
# We can see classes of our edx data set
str(edx)
```

Now we can explore our edx data set. 
Let's check the number of unique movies and users in edx data set

```{r check edx unique movies and users, echo=FALSE, message=FALSE}
# We can see the number of unique movies and users in edx set
edx %>%
  summarise(n_users = n_distinct(userId), n_movies = n_distinct(movieId))
```

Let's see some examples of top movie genre list in edx data set

```{r check edx top genre list, echo=FALSE, message=FALSE}
# We can see some examples of top movie genre list in edx set
edx %>% group_by(genres) %>% 
  summarise(n=n()) %>%
  arrange(desc(n)) %>%
  head()
```

Let's see the number of some popular genre movies in edx data set

```{r check edx top genre quantity, echo=FALSE, message=FALSE}
# We can see the quantity of some popular genre movies in edx set
genres = c("Action", "Adventure", "Children", "Comedy", "Drama", "Romance", "Sci-Fi", "Thriller")
sapply(genres, function(g) {
  sum(str_detect(edx$genres, g))
})
```

Let's check out some most rated movies in edx data set

```{r check edx most rated movies, echo=FALSE, message=FALSE}
# We can see some of the most rated movies in edx set
edx %>% group_by(movieId, title) %>%
  summarise(count = n()) %>%
  arrange(desc(count))
```

Let's check out the ratings of all movies in edx data set

```{r check edx rating summary, echo=FALSE, message=FALSE}
# We can see the rating summary of all movies in edx set
edx %>% group_by(rating) %>% summarise(n=n()) %>% arrange(desc(rating))
```

Let's visually see the ratings distribution of all movies in a chart

```{r check edx rating distribution, echo=FALSE, message=FALSE}
# We can visually see the distribution of movie ratings in edx set
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "http://cran.us.r-project.org")
library(ggthemes)
library(scales)
edx %>%
  group_by(rating) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = rating, y = count)) +
  geom_line() +
  scale_y_continuous(labels = comma) + 
  ggtitle("Movie Rating Distribution", subtitle = "Higher ratings are more common") + 
  xlab("Rating") +
  ylab("Count") +
  theme_economist()
```

Let's visually see the distribution of number of ratings by number of movies

```{r check edx rating vs movie distribution, echo=FALSE, message=FALSE}
# We can visually see the distribution of number of ratings by number of movies in edx set
edx %>% group_by(movieId) %>%
  summarise(n=n()) %>%
  ggplot(aes(n)) +
  geom_histogram(color = "black") +
  scale_x_log10() + 
  ggtitle("Distribution of Number of Ratings by Number of Movies", 
          subtitle = "The distribution is almost like normal distribution (quite symetric)") +
  xlab("Number of Ratings") +
  ylab("Number of Movies") + 
  theme_economist()
```

Let's visually see the distribution of number of ratings by number of users

```{r check edx rating vs user distribution, echo=FALSE, message=FALSE}
# We can visually see the distribution of number of ratings by number of users in edx set
edx %>% group_by(userId) %>%
  summarise(n=n()) %>%
  ggplot(aes(n)) +
  geom_histogram(color = "black") +
  scale_x_log10() + 
  ggtitle("Distribution of Number of Ratings by Number of Users", 
          subtitle="The distribution is skewed right (positively skewed)") +
  xlab("Number of Ratings") +
  ylab("Number of Users") + 
  scale_y_continuous(labels = comma) + 
  theme_economist()
```


## 3. Analysis

Now, we are going to create our model evaluation function which is also known as loss function: The residual mean square error (RMSE).
Later, We will decide the best algorithm trained on the training set using the residual mean squared error (RMSE) with cross-validation on the validation set at the end.

```{r RMSE function, echo=FALSE, message=FALSE}
# We create our loss function: The residual mean square error (RMSE)
# We decided the best algorithm based on the residual mean squared error (RMSE) on a test set.
# We can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating. 
# If this number is larger than 1, it means our typical error is larger than one star, which is not good.
# We write a function that computes the RMSE for vectors of ratings and their corresponding predictors:
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

We know that the estimate that minimizes the RMSE is the least squares estimate of mu and, in this case, is the average of all ratings in our training set:

```{r mu, echo=FALSE}
# We know that the estimate that minimizes the RMSE is the least squares estimate of mu and, in this case, is the average of all ratings:
mu <- mean(train_set$rating)
mu
#[1] 3.512456
```

The average of all ratings (mu) is 3.512456

Our first model is just the average model RMSE.
The formula is Yu,i = mu + Eu,i
We obtain the following RMSE:

```{r naive_rmse, echo=FALSE}
# Our first model formula is Yu,i = mu + Eu,i
# If we predict all unknown ratings with mu, we obtain the following RMSE:
naive_rmse <- RMSE(test_set$rating, mu)
naive_rmse
#[1] 1.060054
# Just the average model RMSE is 1.060054 (our first model)
```

Just the average model RMSE is 1.06 (our first model)

We are going to store all our RMSE goal in rmse_result tibble

```{r rmse_results goal, echo=FALSE}
# We are going to store all our model RMSE in rmse_result tibble to compare to our RMSE goal
options(pillar.sigfig = 7)
rmse_results <- tibble(method = "RMSE Goal", RMSE = 0.86490)
rmse_results
```

We are going to store all our model RMSE in rmse_result tibble to compare to our RMSE goal of 0.86490

```{r rmse_results 1, echo=FALSE}
rmse_results <- bind_rows(rmse_results, 
                          tibble(method = "Just the average", 
                                 RMSE = naive_rmse))
rmse_results
```

Our second model: modeling movie effect formula will be adding movie bias as follow: Yu,i = mu + b_i + Eu,i

We can see that these estimates vary substantially in the following chart:

```{r movie effect qplot, echo=FALSE, message=FALSE}
# Our second model: modeling movie effect formula will be adding movie bias as follow: Yu,i = mu + b_i + Eu,i
mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

# We can see that these estimates vary substantially:
qplot(b_i, data = movie_avgs, bins = 10, color = I("black"))
```

Remember mu = 3.5 so a b_i = 1.5 implies a perfect five star rating.
Let's see how much our prediction improves once we use yu,i = mu + b_i:

```{r movie effect rmse, echo=FALSE, message=FALSE}
# Remember mu = 3.5 so a b_i = 1.5 implies a perfect five star rating.
# Let's see how much our prediction improves once we use yu,i = mu + b_i:
predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
Meffect_rmse <- RMSE(predicted_ratings, test_set$rating)
Meffect_rmse
# [1] 0.9429615
# Movie effect model RMSE is 0.9429615 (our second model)
```

The Movie effect model RMSE is 0.944 (our second model)

Here is the results of our method and RMSE so far:

```{r rmse_results 2, echo=FALSE}
rmse_results <- bind_rows(rmse_results, 
                          tibble(method = "Movie effect model", 
                                 RMSE = Meffect_rmse))
rmse_results
```

For our third model, Movie + User effect model, let's start with computing the average rating for user u for those that have rated over 100 movies.
Here is the chart plot:

```{r movie_user effect plot, echo=FALSE, message=FALSE}
# Let's compute the average rating for user u for those that have rated over 100 movies:
train_set %>% 
  group_by(userId) %>% 
  summarise(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")
```

Our third model: modeling movie + user effect formula will be adding user bias as follow: Yu,i = mu + b_i + b_u + Eu,i
We will compute an approximation by computing mu and b_i and estimating b_u as the average of yu,i - mu - b_i.
Then We can now construct predictors and see how much the RMSE improves using our User effect model:

```{r movie_user effect rmse, echo=FALSE, message=FALSE}
# Our third model: modeling movie + user effect formula will be adding user bias as follow: Yu,i = mu + b_i + b_u + Eu,i
# We will compute an approximation by computing mu and b_i and estimating b_u as the average of yu,i ??? mu ??? b_i:
user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarise(b_u = mean(rating - mu - b_i))

# We can now construct predictors and see how much the RMSE improves using our User effect model:
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
MUeffect_rmse <- RMSE(predicted_ratings, test_set$rating)
MUeffect_rmse
# [1] 0.8646843
# Movie + User Effect model RMSE is 0.8646843 (our third model)
```


Movie + User Effect model RMSE is 0.8646843 (our third model)

Here is the results of our method and RMSE so far:

```{r rmse_results 3, echo=FALSE}

rmse_results <- bind_rows(rmse_results, 
                          tibble(method = "Movie + User Effect model", 
                                 RMSE = MUeffect_rmse))
rmse_results
```

For our fourth model, we are going to add regularization to improve our predicted RMSE.
We can use regularization for the estimate movie + user effects as well. 
We are minimizing:
 Sum ( (yu,i - mu - bi - bu)^2 + lambda(sum(bi^2) + sum(bu^2)) )

Our fourth model: Regularized Movie + User Effect Model

Choosing the best penalty terms. Lambda is a tuning parameter. 
We can use cross-validation to choose it to get our optimized lambda.
Here is the plot of lambda by increment of 0.25 from 0 to 10, and optimized lambda value.

```{r lambda, echo=FALSE, message=FALSE}

# Our fourth model: Regularized Movie + User Effect Model
# We can use regularization for the estimate user effects as well. We are minimizing:
# Sum ( (yu,i - mu - bi - bu)^2 + lambda(sum(bi^2) + sum(bu^2)) )
# Choosing the best penalty terms. Lambda is a tuning parameter. We can use cross-validation to choose it.

lambdas <- seq(0, 10, 0.25)

rmses <- sapply(lambdas, function(l){
  
  mu <- mean(train_set$rating)
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_i - mu)/(n()+l))
  
  predicted_ratings <- test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, test_set$rating))
})

#For the full model, the optimal lambda is:
qplot(lambdas, rmses)
lambda <- lambdas[which.min(rmses)]
lambda
# [1] 5
```

Using our training set and cross-validating it with our test set, we get our optimized lambda at 5.

Let's compute our regularized movie + user effect model.

```{r regularized movie user effect model, echo=FALSE, message=FALSE}
# Let's compute regularized movie + user effect model
lambda <- 5

mu <- mean(train_set$rating)

b_i <- train_set %>% 
  group_by(movieId) %>%
  summarise(b_i = sum(rating - mu)/(n()+lambda))

b_u <- train_set %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarise(b_u = sum(rating - b_i - mu)/(n()+lambda))

predicted_ratings <- test_set %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

GMUeffect_rmse <- RMSE(predicted_ratings, test_set$rating)
GMUeffect_rmse
# Regularized Movie + User Effect Model RMSE is 0.8641362 (our fourth model)
```

Regularized Movie + User Effect Model RMSE is 0.8641362 (our fourth model)

Here is the results of our method and RMSE so far:

```{r rmse_results 4, echo=FALSE}
rmse_results <- bind_rows(rmse_results, 
                          tibble(method = "Regularized Movie + User Effect model", 
                                 RMSE = GMUeffect_rmse))
rmse_results
```

Our goal is final model RMSE < 0.86490, so we meet the goal with our final model RMSE as 0.8641362 < 0.86490

Let's compute our final regularized movie + user effect model on edx data set and cross-validate it using validation set.

```{r final model edx vs validation, echo=FALSE, message=FALSE}
# Final predicted_ratings using edx and validation set
# Let's compute regularized movie + user effect model
lambda <- 5

mu <- mean(edx$rating)

b_i <- edx %>% 
  group_by(movieId) %>%
  summarise(b_i = sum(rating - mu)/(n()+lambda))

b_u <- edx %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarise(b_u = sum(rating - b_i - mu)/(n()+lambda))

predicted_ratings <- validation %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

GMUeffect_validation_rmse <- RMSE(predicted_ratings, validation$rating)
GMUeffect_validation_rmse
# Regularized Movie + User Effect Model RMSE is 0.8648177 (our fourth model)
```

Here is the results of all our methods and RMSEs:

```{r rmse_results 5, echo=FALSE}
# RMSE (25 points): RMSE < 0.86490, so we should get full 25 points as 0.8648177 < 0.86490
rmse_results <- bind_rows(rmse_results, 
                          tibble(method = "Final Regularized Movie + User Effect model (edx vs validation)", 
                                 RMSE = GMUeffect_validation_rmse))
rmse_results
```

Our goal is final model RMSE < 0.86490, as 0.8648177 < 0.86490 so we meet the goal with our final model RMSE using edx data set with cross-validation using validation data set,


## 4. Results

Our first model: Just the average model has RMSE at 1.06
The formula is Yu,i = mu + Eu,i
This sets as our baseline

Our second model: movie effect model has RMSE at 0.944   
The formula is Yu,i = mu + b_i + Eu,i
The Movie effect model RMSE has made a little improvement over our first baseline model, just the average model (RMSE 0.944 < RMSE 1.06)

Our third model: movie + user effect model has RMSE at 0.865   
The formula is Yu,i = mu + b_i + b_u + Eu,i
The Movie + User Effect model RMSE had made a better improvement over our second model, Movie Effect Model (RMSE 0.865 < RMSE 0.944)

Our fourth model: Regularized Movie + User Effect Model has RMSE at 0.864817
The formula is Sum ( (yu,i - mu - bi - bu)^2 + lambda(sum(bi^2) + sum(bu^2)) )
Regularized Movie + User Effect Model RMSE has made the best improvement over our third model, Movie + User Effect Model (RMSE 0.864817 < RMSE 0.865)

Our goal is to have a final RMSE < 0.86490, so our final model, Regularized Movie + User Effect Model has met our goal with RMSE 0.864817 < goal RMSE 0.86490

Here is the results of all our methods and RMSEs:

```{r rmse_results 6, echo=FALSE}
# RMSE (25 points): RMSE < 0.86490, so we should get full 25 points as 0.864817 < 0.86490
rmse_results
```

Our goal is final model RMSE < 0.86490, as 0.864817 < 0.86490 so we meet the goal with our final model RMSE using edx data set with cross-validation using validation data set,


## 5. Conclusion

Summary of the results:

```{r rmse_results 7, echo=FALSE}
# RMSE (25 points): RMSE < 0.86490, so we should get full 25 points as 0.864817 < 0.86490
rmse_results
```

The Goal to beat RMSE 0.86490 has been met using Regularized Movie + User Effect Model.
With that results, we can say that the final model can predict the movie ratings as close as the true ratings in the validation set.


#### Limitations
1. Some machine learning algorithms are computationally expensive to run in a commodity laptop. The required amount of memory to compute far exceeded the available memory in a commodity laptop, even with increased virtual memory.

2. Only two predictors are used, the movie and user data, not considering other features. Modern recommendation system models use many predictors, such as genres, bookmarks, playlists, etc.

3. The model works only for existing users, movies and rating values, so the algorithm must run every time a new user or movie is included, or when the rating changes. This is not an issue for small client base and a few thousand movies, but may become a concern for large data sets. The model should consider these changes and update the predictions as information changes.

4. There is no initial recommendation for a new user or for users that usually don’t rate movies. Algorithms that uses several features as predictors can overcome this issue.


#### Future Work
This report briefly describes simple models that predicts ratings. There are a few widely adopted approaches not discussed here: matrix-factorization, content-based and collaborative filtering. 

The recommenderlab package implements these methods and provides an environment to build and test recommendation systems.

Besides recommenderlab, there are other packages for building recommendation systems available in The Comprehensive R Archive Network (CRAN) website.



## References
Rafael A. Irizarry (2019), Introduction to Data Science: Data Analysis and Prediction Algorithms with R

https://www.edx.org/professional-certificate/harvardx-data-science↩

https://www.netflixprize.com/↩

https://grouplens.org/↩

https://movielens.org/↩

https://grouplens.org/datasets/movielens/latest/↩

https://grouplens.org/datasets/movielens/10m/↩

https://cran.r-project.org/web/packages/available_packages_by_name.html↩
